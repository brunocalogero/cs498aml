---
output:
  html_document:
    theme: readable
    toc: yes
---
```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
options(scipen=1, digits=4, width=80)
library(faraway)
library(readr)
library(caret)
library(klaR)
library(lattice)
library(glmnet)
library(plotmo)
library(gridExtra)
```

# CS 498 AML Homework 5
## Question 1 - Music
```{r read_music, message=FALSE}
music <- read_csv('./music.csv')
music.num_features = dim(music)[2]-2
music["latitude"] <- music["latitude"] + 90
music["longitude"] <- music["longitude"] + 180
```
A small angle transformation was required here so as to allow us to use the box-cox transformation correctly.

### Latitude
**(a)** Fit a linear model

```{r lat_lm}
lat.lm <- lm(latitude ~ . - longitude, data = music)
```

```{r lat_lm_info, echo=FALSE}
print(c("The R^2 value is", summary(lat.lm)$r.squared))
par(mfrow=c(2,2))
plot(lat.lm)
```
A natural measure of the goodness of a regression is what percentage of the variance of y it explains. Essentially, the Rsquared value tells us how well the regression explains the training data. Good predictions result in high values of Rsquared, and a perfect model will have Rsquared = 1 (which doesn’t usually happen). In this case our Rsquared estimation is rather low (approx. 0.29) , explicitely telling us that we should consider a better model or perhaps a transformation. Indeed, The Box-Cox transformation is a method that can search for a transformation of the dependent variable that improves the regression.The method uses a one-parameter family of transformations, with parameter λ, then searches for the best value of this parameter using maximum likelihood. One searches for a value of λ that makes residuals look most like a normal distribution.

**(b)** Box-Cox

```{r lat_bc}
lat.bc_info = boxcox(lat.lm, lambda=seq(-1,5))
lat.bc_lambda = lat.bc_info$x[which.max(lat.bc_info$y)] #selecting best lambda
lat.lm_lambda = lm( (latitude^lat.bc_lambda-1)/lat.bc_lambda ~ . - longitude, data=music)
# now let us see if we have improved our model and Rsquared value
```

```{r lat_bc_info, echo=FALSE}
par(mfrow=c(2,2))
plot(lat.lm_lambda)
print(c("The new R^2 value is", summary(lat.lm_lambda)$r.squared))
print(c("The Box-Cox optimal lambda is", lat.bc_lambda))
```
Moreover, we have a new $R^2$ value that has increased to approx. $0.32$ which clearly demonstrates an improvement from the original $0.29$. Hence we will be using the box-cox model for the rest of the excercise. We now proced to regularization, and more specifically Ridge and Lasso regularizations, we want to see if we can still improve things at this point.

**(c)** Regularization
```{r lat_lasso}
# Lasso L1
lat.l1 = glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["latitude"]^lat.bc_lambda),
				alpha=1)
lat.l1_cv = cv.glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["latitude"]^lat.bc_lambda),
				alpha=1)
```
```{r lat_lasso_info, echo=FALSE}
par(mfrow=c(1,2))
plot(lat.l1, xvar="lambda")
plot(lat.l1_cv)
```


```{r lat_ridge}
# Ridge L2
lat.l2 = glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["latitude"]^lat.bc_lambda),
				alpha=0)
lat.l2_cv = cv.glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["latitude"]^lat.bc_lambda),
				alpha=0)
```
```{r lat_ridge_info, echo=FALSE}
par(mfrow=c(1,2))
plot(lat.l2, xvar="lambda")
plot(lat.l2_cv)
```


### Longitude
**(a)** Fit a linear model
```{r lon_lm}
lon.lm <- lm(longitude ~ . - latitude, data = music)
```
```{r lon_lm_info, echo=FALSE}
print(c("The R^2 value is", summary(lon.lm)$r.squared))
par(mfrow=c(2,2))
plot(lon.lm)
```

**(b)** Box-Cox

```{r lon_bc}
lon.bc_info = boxcox(lon.lm)
lon.bc_lambda = lon.bc_info$x[which.max(lon.bc_info$y)]
lon.lm_lambda = lm((longitude^lon.bc_lambda-1)/lon.bc_lambda ~ . - latitude, data=music)
```
```{r lon_bc_info, echo=FALSE}
print(c("The new R^2 value is", summary(lon.lm_lambda)$r.squared))
print(c("The Box-Cox optimal lambda is", lon.bc_lambda))
par(mfrow=c(2,2))
plot(lon.lm_lambda)
```

The Box-Cox transformation does not help becuase the residuals are not really effected. Since the optimal $\lambda$ found by Box-Cox is about $1$, we don't expect this to do a lot.

**(c)** Regularization
```{r lon_lasso}
# Lasso L1
lon.l1 = glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["longitude"]^lon.bc_lambda),
				alpha=1)
lon.l1_cv = cv.glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["longitude"]^lon.bc_lambda),
				alpha=1)
```
```{r lon_lasso_info, echo=FALSE}
par(mfrow=c(1,2))
plot(lon.l1, xvar="lambda")
plot(lon.l1_cv)
```

```{r lon_ridge}
# Ridge L2
lon.l2 = glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["longitude"]^lon.bc_lambda),
				alpha=0)
lon.l2_cv = cv.glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["longitude"]^lon.bc_lambda),
				alpha=0)
```
```{r lon_ridge_info, echo=FALSE}
print(c("the minimum  lambda is :", log(lat.l1_cv$lambda.min)))
par(mfrow=c(1,2))
plot(lon.l2, xvar="lambda")
plot(lon.l2_cv)
```

```{r}
plot(log(lat.l1_cv$lambda),lat.l1_cv$cvm,pch=19,col="red",xlab="log(Lambda)",ylab=lat.l1_cv$name)
points(log(lat.l2_cv$lambda),lat.l2_cv$cvm,pch=19,col="grey")
legend("topleft",legend=c("alpha= 1 L1","alpha= 1 L2"),pch=19,col=c("red","grey"))
```


## Question 2 - Default
```{r read_default, message=FALSE}
default <- read_csv('./default.csv')
default.num_features = dim(default)[2]-1
```
**(a)** Fit a linear model
```{r default_lm}
default.lm_cv <- cv.glmnet(as.matrix(default[1:default.num_features]),
						as.matrix(default["default"]),
						type.measure = "auc",
						family="binomial")
default.lm_cv.lasso <- cv.glmnet(as.matrix(default[1:default.num_features]),
						as.matrix(default["default"]),
						family="binomial",
						type.measure = "auc",
						alpha=1)
default.lm_cv.ridge <- cv.glmnet(as.matrix(default[1:default.num_features]),
						as.matrix(default["default"]),
						family="binomial",
						type.measure = "auc",
						alpha=0)
default.lm_cv.elastic<- cv.glmnet(as.matrix(default[1:default.num_features]),
						as.matrix(default["default"]),
						family="binomial",
						type.measure = "auc",
						alpha=0.5)
plot(default.lm_cv)
plot(default.lm_cv.lasso, sub="Lasso")
plot(default.lm_cv.ridge, sub="Ridge")
plot(default.lm_cv.elastic,sub="Elasticnet")
```


## Question 3 - Cancer
```{r read_cancer, message=FALSE}
cancer <- read_csv('./cancer.csv', col_names = FALSE)
cancer.num_features = dim(cancer)[2]-1
colnames(cancer)[2001] <- 'state'

# AUC
cancer.lm_cv.lasso.auc <- cv.glmnet(model.matrix(~ ., data=cancer[1:cancer.num_features]),
						cancer$state,
						family="binomial",
						type.measure = "auc",
						alpha=1, nfolds=5)
plot(cancer.lm_cv.lasso.auc)

# Deviance
cancer.lm_cv.lasso.deviance <- cv.glmnet(model.matrix(~ ., data=cancer[1:cancer.num_features]),
						cancer$state,
						family="binomial",
						type.measure = "deviance",
						alpha=1)
plot(cancer.lm_cv.lasso.deviance)
```

Both measures uses about the same lambda. Regarding the number of genes, we realized that the deviance measure reported a slightly higher number than AUC when we ran it multiple times. AUC uses about 10 genes, but deviance uses about 15-20 genes. 