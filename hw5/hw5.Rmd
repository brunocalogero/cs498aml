---
output:
  html_document:
    theme: readable
    toc: yes
---
```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
options(scipen=1, digits=4, width=80)
library(faraway)
library(readr)
library(caret)
library(klaR)
library(lattice)
library(glmnet)
library(plotmo)
library(gridExtra)
```

# CS 498 AML Homework 5
## Question 1 - Music
```{r read_music, message=FALSE}
music <- read_csv('./music.csv')
music.num_features = dim(music)[2]-2
music["latitude"] <- music["latitude"] + 90
music["longitude"] <- music["longitude"] + 180
```
A small angle transformation was required here to make the $latitude$ and $longitude$ values non-negative. Box-Cox requires the response variables to be non-negative.

### Latitude
**(a)** Fit a linear model

```{r lat_lm}
lat.lm <- lm(latitude ~ . - longitude, data = music)
```

```{r lat_lm_info, echo=FALSE}
print(c("The R^2 value is", summary(lat.lm)$r.squared))
xyplot(lat.lm$residuals~ lat.lm$fitted.values,
      xlab = "Fitted Values",
      ylab = "Residuals",
      main = "Residual Diagnostic Plot",
	  sub = "Original scale",
      panel = function(x, y, ...)
      {
        panel.grid(h = -1, v = -1)
        panel.abline(h = 0)
        panel.xyplot(x, y, ...)
      }
)
```

The $R^2$ value tells us how well the regression explains the training data. 
In this case our $R^2$ estimation is rather low (approx. $0.29$), explicitely telling us that we should consider a better model or perhaps a transformation. 
Indeed, The Box-Cox transformation is a method that can search for a power transformation of the dependent variable that improves the regression.
One searches for a value of $\lambda$ that makes residuals look most like a normal distribution.

**(b)** Box-Cox

```{r lat_bc}
lat.bc_info = boxcox(lat.lm, lambda=seq(-1,5))
lat.bc_lambda = lat.bc_info$x[which.max(lat.bc_info$y)] #selecting best lambda
lat.lm_lambda = lm( latitude^lat.bc_lambda ~ . - longitude, data=music)
# now let us see if we have improved our model and Rsquared value
```
Above, different powers ($\lambda$) are teseted on the response variable and the one with the highest log-likelihood is chosen.

```{r lat_bc_info, echo=FALSE}
xyplot((lat.lm_lambda$residuals)^(1/lat.bc_lambda) ~ (lat.lm_lambda$fitted.values)^(1/lat.bc_lambda),
      xlab = "Fitted Values",
      ylab = "Residuals",
      main = "Residual Diagnostic Plot",
	  sub = toString(c("Power scale", lat.bc_lambda)),
      panel = function(x, y, ...)
      {
        panel.grid(h = -1, v = -1)
        panel.abline(h = 0)
        panel.xyplot(x, y, ...)
      }
)
print(c("The new R^2 value is", summary(lat.lm_lambda)$r.squared))
print(c("The Box-Cox optimal lambda is", lat.bc_lambda))
```
Moreover, we have a new $R^2$ value that has increased to approx. $0.32$ which clearly demonstrates an improvement from the original $0.29$. Hence we will be using the Box-Cox model for the rest of the excercise.
This is a significant enough increase in accuracy to justify using a more complex model.
We now proced to regularization, and more specifically Ridge and Lasso regularizations, we want to see if we can still improve things at this point.

Looking at the residuals vs fitted graph (transformed back to the orignal power), there appears to be a pattern. 
We expect the residuals to be normally distirbuted, this should make us question out assumption of homoscedasticity. 

**(c)** Regularization
```{r lat_reg}
# Lasso L1
lat.l1 = cv.glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["latitude"]^lat.bc_lambda),
				alpha=1)
# Ridge L2
lat.l2 = cv.glmnet(as.matrix(music[1:music.num_features]),
				   as.matrix(music["latitude"]^lat.bc_lambda),
				   alpha=0)
```


```{r lat_ridge_info, echo=FALSE}
print(c("The minimum log lambda for Lasso L1 is", log(lat.l1$lambda.min)))
print(c("The minimum log lambda for Ridge L2 is", log(lat.l2$lambda.min)))
plot(lat.l1, main="Lasso L1")
plot(lat.l2, main="Ridge L2")
```

```{r, eval=FALSE}
plot(log(lat.l1$lambda),
	 (lat.l1$cvm)^(1/lat.bc_lambda),
	 pch=19,
	 col="red",
	 xlab="log(Lambda)",
	 ylab="CVM",
	 main="MSE as function of log(Lambda)",
	 sub = toString(c("Lasso L1", "Power scale", lat.bc_lambda)))
points(log(lat.l2$lambda),
	   (lat.l2$cvm)^(1/lat.bc_lambda),
	   pch=19,
	   col="blue")
legend("bottomright", legend=c("alpha=1 L1","alpha=0 L2"), pch=10, col=c("red", "blue"))
```


### Longitude
**(a)** Fit a linear model
```{r lon_lm}
lon.lm <- lm(longitude ~ . - latitude, data = music)
```
```{r lon_lm_info, echo=FALSE}
print(c("The R^2 value is", summary(lon.lm)$r.squared))
xyplot(lon.lm$residuals ~ lon.lm$fitted.values,
      xlab = "Fitted Values",
      ylab = "Residuals",
      main = "Residual Diagnostic Plot",
	  sub = "Original scale",
      panel = function(x, y, ...)
      {
        panel.grid(h = -1, v = -1)
        panel.abline(h = 0)
        panel.xyplot(x, y, ...)
      }
)
```

**(b)** Box-Cox

```{r lon_bc}
lon.bc_info = boxcox(lon.lm)
lon.bc_lambda = lon.bc_info$x[which.max(lon.bc_info$y)]
lon.lm_lambda = lm(longitude^lon.bc_lambda ~ . - latitude, data=music)
```
```{r lon_bc_info, echo=FALSE}
print(c("The new R^2 value is", summary(lon.lm_lambda)$r.squared))
print(c("The Box-Cox optimal lambda is", lon.bc_lambda))
xyplot((lon.lm_lambda$residuals)^(1/lon.bc_lambda) ~ (lon.lm_lambda$fitted.values)^(1/lon.bc_lambda),
      xlab = "Fitted Values",
      ylab = "Residuals",
      main = "Residual Diagnostic Plot",
	  sub = toString(c("Power scale", lat.bc_lambda)),
      panel = function(x, y, ...)
      {
        panel.grid(h = -1, v = -1)
        panel.abline(h = 0)
        panel.xyplot(x, y, ...)
      }
)
```

The Box-Cox transformation does not help becuase the residuals are not really effected.
Since the optimal $\lambda$ found by Box-Cox is about $1$, we don't expect this to do a lot.
Since $\lambda$ is about $1$, we will use the simpler model.

**(c)** Regularization
```{r lon_reg}
# Lasso L1
lon.l1 = cv.glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["longitude"]),
				alpha=1)
# Ridge L2
lon.l2 = cv.glmnet(as.matrix(music[1:music.num_features]),
				as.matrix(music["longitude"]),
				alpha=0)
```
```{r lon_reg_info, echo=FALSE}
print(c("The minimum log(lambda) for Lasso L1 is", log(lon.l1$lambda.min)))
print(c("the minimum log(lambda) for Ridge L2 is", log(lon.l2$lambda.min)))
plot(lon.l1, main="Lasso L1")
plot(lon.l2, main="Ridge L2")
```

```{r, eval=FALSE}
plot(log(lat.l1$lambda),lat.l1$cvm,pch=19,col="red",xlab="log(Lambda)",ylab=lat.l1$name)
points(log(lat.l2$lambda),lat.l2$cvm,pch=19,col="grey")
legend("topleft",legend=c("alpha= 1 L1","alpha= 1 L2"),pch=19,col=c("red","grey"))
```


## Question 2 - Default
```{r read_defult, message=FALSE}
default <- read_csv('./default.csv')
default.num_features = dim(default)[2]-1
```
**(a)** Fit a linear model
```{r default_lm, eval=FALSE}
default.lm_cv <- cv.glmnet(as.matrix(default[1:default.num_features]),
						as.matrix(default["default"]),
						type.measure = "auc",
						family="binomial")
default.lm_cv.lasso <- cv.glmnet(as.matrix(default[1:default.num_features]),
						as.matrix(default["default"]),
						family="binomial",
						type.measure = "auc",
						alpha=1)
default.lm_cv.ridge <- cv.glmnet(as.matrix(default[1:default.num_features]),
						as.matrix(default["default"]),
						family="binomial",
						type.measure = "auc",
						alpha=0)
default.lm_cv.elastic<- cv.glmnet(as.matrix(default[1:default.num_features]),
						as.matrix(default["default"]),
						family="binomial",
						type.measure = "auc",
						alpha=0.5)
plot(default.lm_cv)
plot(default.lm_cv.lasso, sub="Lasso")
plot(default.lm_cv.ridge, sub="Ridge")
plot(default.lm_cv.elastic,sub="Elasticnet")
```


## Question 3 - Cancer
```{r read_cancer, message=FALSE}
cancer <- read_csv('./cancer.csv', col_names = FALSE)
cancer.num_features = dim(cancer)[2]-1
colnames(cancer)[2001] <- 'state'

# AUC
cancer.lm_cv.lasso.auc <- cv.glmnet(model.matrix(~ ., data=cancer[1:cancer.num_features]),
						cancer$state,
						family="binomial",
						type.measure = "auc",
						alpha=1, nfolds=5)
plot(cancer.lm_cv.lasso.auc)

# Deviance
cancer.lm_cv.lasso.deviance <- cv.glmnet(model.matrix(~ ., data=cancer[1:cancer.num_features]),
						cancer$state,
						family="binomial",
						type.measure = "deviance",
						alpha=1)
plot(cancer.lm_cv.lasso.deviance)
```

Both measures uses about the same $\lambda$ Regarding the number of genes, we realized that the deviance measure reported a slightly higher number than AUC when we ran it multiple times. AUC uses about 10 genes, but deviance uses about 15-20 genes. 
